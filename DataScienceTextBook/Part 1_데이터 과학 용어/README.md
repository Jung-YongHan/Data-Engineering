# 데이터 사이언스 교과서

## 6. 용어의 차이

### 1.6.1 설명 변수/목적변수, 입력/출력     

통계, 패턴 인식 분야에서는 설명 변수 x, 목적변수 y를 이용.(다양한 분야에서 다르게 표현)  
파이썬 statsmodels의 문서에서는 외생변수, 내생변수의 머리글자를 취해 exog, endog로 표현

    설명 변수 x와 목적변수 y의 다른 표현
    설명 변수(explanatory variable)     |    목적변수(objective variable)
    예측변수(predictor variable)        |    결과변수(outcome variable)
    독립변수(independent variable)      |    종속변수(dependent variable)
    외생변수(exogenous variable)        |    내생변수(endogenous variable)

시스템 공학분야 **(전기전자, 기계, 정보, 건축토목, 물리, 화학, 기상학, 우주과학, 바이오 공학 등의 이공학 분야)** 에서는 입력/출력이라는 표현을 사용.  

***시스템(system)** - 단순한 기능이든 복자한 기능이든 상관없이 어떠한 기능을 가지는 요소가 조합된 것으로 새로운 기능이 드러나게 되는 것  

***역학(dynamics)** - 시간 인자't'를 포함하여 시스템 출력이나 상태가 함께 변동하는 시스템 (=동적 시스템)

### 1.6.2 표본과 데이터
표본(sample)의 의미로 샘플이라는 용어도 보급되어 있음. 표본화(sampling)와 샘플링도 동일  
표본수(샘플수, the number of sample)의 동의어 데이터수(the number of data), 데이터 세트의 수

### 1.6.3 예측과 추정
'예측한다'에 해당하는 영어 predict, forecast  
영영사전에 있는 의미 - "to say that an event or action will happen in the future, sepecially as a result of knowledge or experience."  

문제는 'in the future'의 해석이 분야에 따라 다르다는 점.

동적이지 않은 대상을 다루는 분야(통계, 패턴 인식, 기계 재료 계열 등의 분야)는 미지의 것을 이제부터(future) 찾아내어 알아맞히는(predict에는 foretell의 의미도 있다.) 경우에 predict를 사용하는 경우가 많다.  
ex) 중회귀분석 분야에서는 x를 기반으로 y를 구하는 것을 predict라고 함.

동적인 대상을 다루는 시스템 공학에서는 대상이 되는 시스템 자체에 시간 인자't'를 포함하기 위해 시간축을 강하게 의식하여 현재 시각을 기준으로 과거와 미래를 구별하여 생각.  
이것은 적분을 다룰 때에 구간을 의식해야 하기 때문. 현재와 과거는 관측값을 얻을 수 있지만 미래의 관측값은 얻을 수 없다.  

이 차이로  시스템 상태의 분석,해석이 다른 지점에서 미래(future)의 상태(state)를 아는 것을 예측(prediction), 현재와 과거의 관측할 수 없는 상태를 아는 것을 추정(estimation)이라고 구별.

시스템 공학에서는 실제 상태(state, 신호(signal)라고도 함)는 관측 잡음이나 외란의 영향 때문에 직접적으로 관측할 수 없다고 가정하는 경우가 많음. 즉, 실제 상태 != 관측값.  
이 때문에 과거로부터 현재까지의 관측값을 얻는다고 해도, 시스템의 진정한 상태를 알 수 없기 때문에 이것을 알고 싶다는(추정) 요구가 자주 있다.

또한 시간 불변(time-invariant)의 파라미터를 구하는 경우 시간축에 관계없기 때문에 '추정'이라고 한다.

통계 분야에서도 '추정(estimation)'이라는 용어 사용.  
Bret Larget: Estimation and Prediction, Dept, of Botany and Statitics Univ. of Wisconsin, 2007에 의하면
- 평균값 등 모집단의 파라미터를 구하는 것은 estimation
- X를 기초로 Y를 구하는 것을 prediction  

이라고 하는 경우도 있음.

forecast는 시간에 관한 미래의 상태를 아는 것을 말하고 있고 이것은 통계, 패턴 인식 분야에서도 동일한 의미로 사용.

### 1.6.4 클래스 분류
'판별(또는 식별)(discrimination)'이라는 용어. 판별의 원래 의미로는 차이를 인식 또는 지각한다 등이 있다. 따라서 분류한 것을 판별한다고 말할 수 있다. 분류와 판별의 목적을 생각하면 클래스 분류와 판별 분석은 거의 동의어라고 생각해도 좋다.  
다만 다음과 같은 용례가 있다.  
군중 속에서 있는지 없는지 알 수 없는 범인의 얼굴을 (식별, 판별, 분류, 감정)한다. 이 괄호 속에서 적당한 단어를 하나 선택하라는 물음에 어떻게 답하면 좋을까? 이 예에서는 일단 분류는 사용하지 않을 것이다.

### 1.6.6 오버피팅
오버피팅이 등장하는 이유는 수치계산 분야에서 Runge 현상(Runge's phenomenon)이 발생되기 때문.  
이것은 어느 측정점을 다항식 함수로 보간(interpolation, fitting)할 때 차수가 너무 높으면, 측정점 사이에서 크게 진동하는 현상.  
원인으로는 보간하는 알고리즘이 차수에 해당하는 고차의 미분계수를 이용하기 때문에 이 계수가 너무 커서 진동을 야기하는 것으로 알려져 있다.  
비유를 하여 설명하자면, 10x^4의 계수는 10, 이 3차 미분의 계수는 10x4x3x2=240으로 24배가 됨.  
***Runge** 현상의 해설(https://en.wikipedia.org/wiki/Runge%27s_phenomenon)  
***Overfitting** 해설(https://en.wikipedia.org/wiki/Overfitting)

주의할 것은 오버피팅 현상은 패턴 인식, 기계학습 등에서 동적이지 않은 경우를 대상으로 할 때에 생기는 경우가 있다.

시계열 데이터에 대한 ARMA 모델에서는 차수가 높아도 생기지 않는다. 매개변수 추정 알고리즘에 고차의 미분계수를 사용하는 것은 거의 없기 때문이다.

### 1.6.8 변수
변수와 유사한 용어로 변랑이 있다.  
"통계 집단을 이루는 개체가 '담당하고 있는 수량'을 추상화하여 변량(variate)라고 부르는 경우가 많다. 수학의 변수(variable)의 개념에 해당하지만 개체에 따라서 변화하고 물리적, 경제적인 의미를 가진 양이라는 의식이 강하다. 데이터는 변량이 취하는 값(value)이다. 그러나 변량과 데이터는 변수와 변숫값과 같이 혼동되기 십상이고 구별하지 않는 편이 편리. 변량과 변수도 혼동되기 쉬워서 이 사전 내에서도 구별하지 않는 경우가 많다."

### 1.6.9 상관과 공분산 
상관에는 자기상관(auto-dorrelation)과 상호상관(cross-correlation)이 있다. 이것들은 통계학과 신호처리에서 약간의 차이가 있다. 통계학에서는 값을 -1 ~ +1로 정규화시키지만 신호 처리에서는 정규화시키지 않는다. 따라서 동일한 상관이라도 공식은 약간 다르다.  
또한 통계학의 자기공분산(auto-covariance), 상호공분산(cross-covariance)은 신호 처리에서는 각각 상호상관, 자기상관이라고 말하는 경우가 있다.  
***상관**(https://en.wikipedia.org/wiki/Cross-correlation)

---
---
## 7. 수학, 수치계산, 물리의 시작
### 1.7.1 수학의 시작
#### 벡터와 행렬의 표기
요소 수에 따라 3차원 벡터라든가 x E R^3이라고 표현된다(R은 실수 공간).  
패턴 인식 분야에서는 특징 벡터라는 용어가 있다. 특징 벡터를 위의 x로 하는 경우 첫 번째는 색, 두 번째는 크기, 세 번째는 질량이라는 식으로 특징을 할당하고 각각의 수치를 입력한 것을 생각한다. 이렇게 하면 특징 패턴은 정의된 특징 공간 안에서 나타낼 수 있으므로 충분히 벡터라고 부를 수 있다.


    - 오차(error)의 수치적 평가
    
    절대오차(absolute error) = |측정값 - 참값|
    상대오차(relative error) = |(측정값 - 참값) / 참값|
    
    참값은 기준값을 대신한다. 측정값에 10cm의 절대오차가 생긴다고 하더라도, 기준값(또는 참값)이 100m인가 1m인가로 이 오차평가가 크게 달라지므로 상대오차가 몇 %인가로 표현하는 편이 좋은 경우가 많다.


- 놈과 거리  
놈(norm)과 거리(distance, metric)는 매우 유사하기 때문에 이 차이에 유의.  
놈은 여기에선느 벡터의 길이(크기라고 생각해도 좋다)를 말한다. 길이이므로 음의 값은 없다.

Lp놈 - [나무위키](https://namu.wiki/w/%EB%85%B8%EB%A6%84(%EC%88%98%ED%95%99)#s-4.2)참고  
p = 2, p = 1인 경우 L2 놈은 : 유클리드 놈, L1 놈 맨해튼 거리 등으로 사용  
거리는 두 점 사이의 차이를 말한다. 놈이 신장을 나타낼 때 발뒤꿈치의 좌표로부터 머리 꼭 대기까지의 좌표가 거리가 된다. 즉, 거리는 좌표를 의식한다.  
여기에서 거리는 항상 두 점 사이를 연결하는 직선으로 한정되지 않고, 맨해튼 거리와 같이 굽은 측정 방법도 있는 것에 유의.

패턴 인식 등에서는 맨해튼 거리(L1거리, Manhattan distance, taxicab geometry라고도 부른다), 유클리드 거리(Euclidean distance), 마할라노비스 거리(Mahalanobis' distance) 등이 이용.  
맨해튼 거리는 도시의 도로가 바둑판 모양일 때 그 도로를 따라서 거리를 측정하는 것. 이 도시의 도로로부터 유래하여 맨해튼이라는 이름이 지어짐.

### 1.7.2 수치계산의 문제
컴퓨터를 이용하는 계산에서는 숫자가 유한한 자리인 것, 내부 표현이 2진수인 것, 이산 데이터를 다루는 것 등에 기인한 몇 가지 문제 발생.  
- 0.1의 변환 오차  
    10 진수 0.1을 2진수로 변환하면 0.0'0011'인 순환소수가 됨. 컴퓨터의 기억 영역이 유한하기 때문에 어딘가에서 잘라야 함. 이것은 10진수 데이터 0.1을 컴퓨터에 입력한 순간에 오차가 생긴다는 것을 의미.

- 반올림, 정보 손실, 자릿수 손실 오차  

    - 반올림은 사사오입, 절사, 절상 등 어느 자릿수에 대해서 수행되는 것을 그 상위 자릿수로 유효 자릿수를 맞추는 것.  
    당연히 반올림에 수반되는 오차가 생김.  
    또한 반올림은 양의 편향이 생기긱 때문에 컴퓨터 내부 계산에서는 그다지 이용되지 않음.
     
    - 정보손실에 관해서는 예를 들면 기억 영역이 4자릿수까지의 변수로 1,000이 이미 저장되어 있을 때 0.1을 더하더라도 이것을 무시하는 것.  
    예를 들어 100만건의 데이터를 더할 때 1건의 값이 0.1일 때 100만건을 더하더라도 결과가 10만에 모자라게 되는 것이 생기므로 방대한 데이터를 더하는 경우에는 특별한 연구가 필요.

    - 자릿수 손실은 거의 동일한 값을 뺀 결과 유효숫자가 감소하는 경우를 말한다.

- 머신 입실론(machine epsilon)
    컴퓨터가 다루는 실수는 부동소수점수(floating point number)이다.  
    부동소수점수는 IEEE 754 규격을 따르는 경우가 많아 이 규격에서는 수의 분해능(구별, resolution, discrimination)(이웃한 각 수의 거리)이 수의 절댓값에 따라 일정하지 않고, 절댓값이 커지는 만큼 분해능이 저하되고 반대도 마찬가지인 성질을 가진다.  

    **머신 입실론의 두 가지 정의**  
    1. '1보다 큰 최소의 수'와 1과의 차이  
    \- IEEE 754 배정밀도의 경우 52비트 가수부의 최소 비트 정도만이 머신 입실론이 됨.  
    epsilon = 2^(-52) ~= 2.2204 x 10^(-16)

    2. (1 + epsilon) > 1이 참이 되는 최소의 부동소수점수 epsilon.  
    \- 이 경우 부동소수점 수가 2진수로 표현됨으로써 epsilon의 다음 작은 수는 epsilon/2이 되고 (1 + epsilon/2) > 1은 거짓이 됨.  
    [epsilon 확인 코드](https://github.com/Jung-YongHan/Data-Engineering/blob/main/DataScienceTextBook/Part%201_%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EA%B3%BC%ED%95%99%20%EC%9A%A9%EC%96%B4/epsilon.py)

    머신 입실론이 말하는 것은 유효자릿수(IEEE 754에서는 가수부(mantissa)에 해당한다)가 점점 작아질 때 어느 정보다 작은 수는 취급할 수 없다는 것을 의미.  
    반대의 경우도 마찬가지

    이 분해능과 앞의 0.1의 변환문제, 정보손실, 자릿수 손실의 문제 등을 포함하여 생각하면 실수를 다루면 어떠한 오차가 섞여 들어간다는 것을 알 수 있음.  

- 방적식의 연산오차  

    다음 방적식을 x에 관해서 수치계산으로 해를 구하는 것을 고려하자.
    3x = 7  
    초보적인 사고방식은 예를 들면 (1/3) ~= 0.333인 반올림한 숫자로 양변에 나눗셈을 하는 것.  
    이것은 처음의 (1/3)의 계산과 두 번째 나눗셈을 하는 곳에서 오차가 섞이므로 바람직하지 않다.  

    연립방정식에서 가우스 소거법은 이것을 피해 양변의 x의 계수로 나눗셈을 한다. 이렇게 되면 수치계산 상의 오차가 들어갈 기회는 한 번으로 끝난다.  
    이 예는 알고리즘을 주의 깊게 설계하지 않으면, 수치계산 그 자체에 오차가 섞이는 문제가 생긴다는 것을 지적.

- 수렴 판정  
    이 책에서는 우도 함수나 분산의 최소화에 기반을 둔 데이터 분석을 다룬다.  
    이러한 계산 외에도 대규모 행렬 계산을 하면 한 번에 해결할 수 없으므로 반복 계산을 하여 구하게 된다.  
    이 때 수치계산 상의 여러가지 요건(무한정 계산할 수 없으므로)에 의해, 반복계산을 통해 '어느 정도'까지 해에 가까워 졌다고 판정되면 비록 실제의 해에 도달하지 않아도, 그것을 해로 간주하고 계산을 중지한는 알고리즘이 대부분이다.  

    ex) 반복 계산에서 고전적으로 유명한 뉴턴법에서 f(x) = x^2 - 9의 해를 구하는 것.(epsilon.py에 제시)  
    이 알고리즘에서는 초기값 x0 = 1.0으로부터 계산을 시작하여 수렴 판정을 위한 eps(입실론) = 0.01이라고 두면, 수치해(근사해) = 3.000091554138을 구한다. 이것은 반복 계산에서 지난 번의 계산과 이번 계산에서 구한 수치해의 차이가 eps 이내로 수렴하면 계산을 중지함으로써 얻은 결과.  
    또한 반복 횟수는 5회이다. 이 결과 정확한 해인 3(수식을 이론적으로 푼 해)에 오차가 더해진 값이 수치해이다.

    이와 같이 일반적으로 수치해는 실제 해에 오차가 더해진 형태로 출력되는 것을 염두.

- 의사난수의 성질  

    컴퓨터를 이용하는 이상, 여러 가지 확률분포를 따르는 확률 변수를 근사값으로 생성한다.  
    특히, 균일난수, 정규난수의 의사난수는 자주 이용  
    
    유사 균일난수에는 메모리의 비트 길이만큼 정해진 주기성이 있다.  

    ex) 32비트 메모리의 경우 최대 2^32 즉, 43억 개의 균일난수를 생성하면 1주기가 되는 것.  
    다시 말하면 이 길이에서 상관이 있게 되고 확률론에서 설명하고 있는 무상관과는 반대가 됨.  
    그러나 우리들은 43억이라는 유한개수의 제약 속에서 시뮬레이션을 할 수 밖에 없음.  

    게다가 정규난수는 이 유사 균일난수를 이용하여 Box-Muller 법의 근사계산을 이용하여 생성된다. 이 주기성에 추가하여 근사계산도 수행되고 있다는 것을 염두.(https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform)

파이썬 패키지가 제공하는 분석 도구는 앞의 문제를 포함한 수치해를 제시. 이 때문에 수치해는 어디까지나 정확한 해와 다르기 때문에 수치결과를 그대로 받아들이지 않는 것이 중요.  
한편, 수치해는 어떤 종류의 해나 지식을 발견하는 중요한 길잡이이므로 파이썬 패키지를 충분히 활용하는 것이 바람직.